{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre/Post-process data in production environment\n",
    "\n",
    "In model developing process, `TSDataset` is used to preprocess(including feature engineering, data sampling, scaling, ...) the raw data the postprocess the predicted result(majorly unscaling). This post provides a way by which users could replay the preprocessing and postprocessing in production environment(e.g. model serving).\n",
    "\n",
    "In this guide, we will\n",
    "1. Train a TCNForecaster with nyc_taxi datset and export the model in onnx type and scaler.\n",
    "2. Show users how to replay the preprocessing and postprocessing in production environment.\n",
    "3. Evaluate the performance of preprocessing and postprocessing\n",
    "4. More tips about this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecaster developing\n",
    "\n",
    "First let's prepare the data. We will manually download the data to show the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-10-15 17:16:51--  https://raw.githubusercontent.com/numenta/NAB/v1.0/data/realKnownCause/nyc_taxi.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 265771 (260K) [text/plain]\n",
      "Saving to: ‘nyc_taxi.csv’\n",
      "\n",
      "nyc_taxi.csv        100%[===================>] 259.54K   577KB/s    in 0.4s    \n",
      "\n",
      "2022-10-15 17:16:53 (577 KB/s) - ‘nyc_taxi.csv’ saved [265771/265771]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run following\n",
    "!wget https://raw.githubusercontent.com/numenta/NAB/v1.0/data/realKnownCause/nyc_taxi.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we may load the data to pandas dataframe and carry out preprocessing through `TSDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junweid/BigDL/python/chronos/src/bigdl/chronos/data/utils/quality_inspection.py:139: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if missing_value / rows > threshold:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from bigdl.chronos.data import TSDataset\n",
    "\n",
    "# load the data to pandas dataframe\n",
    "df = pd.read_csv(\"nyc_taxi.csv\", parse_dates=[\"timestamp\"])\n",
    "\n",
    "# use nyc_taxi public dataset\n",
    "train_data, _, test_data = TSDataset.from_pandas(df,\n",
    "                                                 dt_col=\"timestamp\",\n",
    "                                                 target_col=\"value\",\n",
    "                                                 repair=False,\n",
    "                                                 with_split=True,\n",
    "                                                 test_ratio=0.1)\n",
    "\n",
    "# create a scaler for data scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# preprocess(generate datetime feature, scale and roll samping)\n",
    "for data in [train_data, test_data]:\n",
    "    data.gen_dt_feature(features=[\"WEEKDAY\", \"HOUR\", \"MINUTES\"])\\\n",
    "        .scale(scaler, fit=(data is train_data))\\\n",
    "        .roll(lookback=48, horizon=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 3551947761\n",
      "Global seed set to 3551947761\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | TemporalConvNet | 5.6 K \n",
      "1 | loss  | MSELoss         | 0     \n",
      "------------------------------------------\n",
      "5.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 K     Total params\n",
      "0.022     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2d2a61897243928f8717535b27d09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bigdl.chronos.forecaster import TCNForecaster  # TCN is algorithm name\n",
    "\n",
    "# create a forecaster\n",
    "forecaster = TCNForecaster.from_tsdataset(train_data)\n",
    "\n",
    "# train the forecaster\n",
    "forecaster.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junweid/BigDL/python/nano/src/bigdl/nano/utils/util.py:29: DeprecationWarning: `bigdl.nano.pytorch.Trainer.trace` will be deprecated in future release. Please use `bigdl.nano.pytorch.InferenceOptimizer.trace` instead.\n",
      "  category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# save the forecaster in onnx type\n",
    "forecaster.export_onnx_file(dirname=\"nyc_tax_onnx_model\", quantized_dirname=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the scaler\n",
    "# There are many ways, we use pickle here\n",
    "with open('scaler.pkl','wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In production environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data to predict in a local csv file\n",
    "_, _, test_data = get_public_dataset(\"nyc_taxi\")\n",
    "test_data.df[-48:].to_csv(\"inference_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "df = pd.read_csv(\"inference_data.csv\", parse_dates=[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_during_deployment(df, scaler):\n",
    "    tsdata = TSDataset.from_pandas(df,\n",
    "                                   dt_col=\"timestamp\",\n",
    "                                   target_col=\"value\",\n",
    "                                   repair=False)\n",
    "    tsdata.gen_dt_feature(features=[\"WEEKDAY\", \"HOUR\", \"MINUTES\"])\\\n",
    "          .scale(scaler, fit=False)\\\n",
    "          .roll(lookback=48, horizon=24, is_predict=True)\n",
    "    data = tsdata.to_numpy()\n",
    "    return tsdata, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_during_deployment(data, tsdata):\n",
    "    return tsdata.unscale_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "session = onnxruntime.InferenceSession(\"nyc_tax_onnx_model/onnx_saved_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdata, data = preprocess_during_deployment(df, scaler)\n",
    "data = session.run(None, {'x': data})[0]\n",
    "processed_data = postprocess_during_deployment(data, tsdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p50': 3.75, 'p90': 4.097, 'p95': 4.228, 'p99': 5.582}\n"
     ]
    }
   ],
   "source": [
    "from bigdl.chronos.metric.forecast_metrics import Evaluator\n",
    "print(Evaluator.get_latency(preprocess_during_deployment, df, scaler))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('chronos')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7cbcfcf124497a723b2fc91b0dad8cd6ed41af955928289a9d3478af9690021"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
